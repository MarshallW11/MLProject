{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "features = pd.read_csv('Features data set.csv')\n",
    "sales = pd.read_csv('sales data-set.csv')\n",
    "stores = pd.read_csv('stores data-set.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "features = features.drop(['IsHoliday'], axis=1)\n",
    "\n",
    "combined = sales.merge(features, on=['Store', 'Date'])\n",
    "combined = combined.merge(stores, on=['Store'])\n",
    "\n",
    "combined.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "combined.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "combined = combined.fillna(value=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "combined.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_df = combined.groupby(['Store', 'Date', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n",
    "                               'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size'], as_index=False).sum()\n",
    "\n",
    "merged_df = merged_df.drop(['Dept'], axis = 1)\n",
    "print(merged_df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'], dayfirst=True)\n",
    "merged_df['Year'] = merged_df['Date'].dt.year\n",
    "merged_df['Month'] = merged_df['Date'].dt.month\n",
    "\n",
    "merged_df['WeekOfYear'] = merged_df['Date'].dt.isocalendar().week\n",
    "merged_df['WeekOfYear'] = merged_df['WeekOfYear'].astype(np.int32)\n",
    "\n",
    "merged_df['DayOfWeek'] = merged_df['Date'].dt.dayofweek\n",
    "\n",
    "merged_df = merged_df.sort_values(by=['Store', 'Date'])\n",
    "print(merged_df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_df['Holiday_Weight'] = 1\n",
    "\n",
    "super_bowl_week = (merged_df['Date'].dt.month == 2) & (merged_df['Date'].dt.isocalendar().week <= 6) & (merged_df['Date'].dt.dayofweek == 6)\n",
    "labor_day_week = (merged_df['Date'].dt.month == 9) & (merged_df['Date'].dt.isocalendar().week == 36)\n",
    "thanksgiving_week = (merged_df['Date'].dt.month == 11) & (merged_df['Date'].dt.isocalendar().week == 47)\n",
    "christmas_period = (merged_df['Date'].dt.month == 12) & (merged_df['Date'].dt.isocalendar().week <= 24)\n",
    "\n",
    "merged_df.loc[super_bowl_week, 'Holiday_Weight'] = 5\n",
    "merged_df.loc[labor_day_week, 'Holiday_Weight'] = 5\n",
    "merged_df.loc[thanksgiving_week, 'Holiday_Weight'] = 5\n",
    "merged_df.loc[christmas_period, 'Holiday_Weight'] = 5\n",
    "\n",
    "merged_df['Weighted_Weekly_Sales'] = merged_df['Weekly_Sales'] * merged_df['Holiday_Weight']\n",
    "\n",
    "merged_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "label_encoder = LabelEncoder()\n",
    "merged_df['Type'] = label_encoder.fit_transform(merged_df['Type'])\n",
    "merged_df.info()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numeric_df = merged_df.select_dtypes(include=[np.number]).drop(columns=['Weekly_Sales', 'Weighted_Weekly_Sales', 'Holiday_Weight', ])\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation = numeric_df.corrwith(merged_df['Weekly_Sales']).sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlation of each feature with Weighted_Weekly_Sales:\")\n",
    "print(correlation)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.heatmap(correlation.to_frame(), annot=True, cmap=\"coolwarm\", cbar=True, fmt=\".2f\")\n",
    "\n",
    "plt.title(\"Correlation with Weighted Weekly Sales\")\n",
    "\n",
    "plt.xlabel(\"Weighted Weekly Sales\")\n",
    "\n",
    "plt.ylabel(\"Features\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pca = PCA()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#making a copy to preserve the original unscaled df\n",
    "combined_scaled = merged_df.copy()\n",
    "\n",
    "#scaling each of the numerical columns\n",
    "combined_scaled['Weekly_Sales'] = scaler.fit_transform(combined_scaled[['Weekly_Sales']])\n",
    "combined_scaled['Temperature'] = scaler.fit_transform(combined_scaled[['Temperature']])\n",
    "combined_scaled['Fuel_Price'] = scaler.fit_transform(combined_scaled[['Fuel_Price']])\n",
    "combined_scaled['MarkDown1'] = scaler.fit_transform(combined_scaled[['MarkDown1']])\n",
    "combined_scaled['MarkDown2'] = scaler.fit_transform(combined_scaled[['MarkDown2']])\n",
    "combined_scaled['MarkDown3'] = scaler.fit_transform(combined_scaled[['MarkDown3']])\n",
    "combined_scaled['MarkDown4'] = scaler.fit_transform(combined_scaled[['MarkDown4']])\n",
    "combined_scaled['MarkDown5'] = scaler.fit_transform(combined_scaled[['MarkDown5']])\n",
    "combined_scaled['CPI'] = scaler.fit_transform(combined_scaled[['CPI']])\n",
    "combined_scaled['Unemployment'] = scaler.fit_transform(combined_scaled[['Unemployment']])\n",
    "combined_scaled['Size'] = scaler.fit_transform(combined_scaled[['Size']])\n",
    "combined_scaled['Weighted_Weekly_Sales'] = scaler.fit_transform(combined_scaled[['Weighted_Weekly_Sales']])\n",
    "combined_scaled['Type'] = scaler.fit_transform(combined_scaled[['Type']])\n",
    "combined_scaled.head()\n",
    "\n",
    "#performing PCA on the numerical features. Which features we select will likely need tweaking\n",
    "#pca.fit(combined_scaled[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Size', 'Weighted_Weekly_Sales']])\n",
    "\n",
    "pca.fit(combined_scaled[['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Size', 'Type']])\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define features and target variable\n",
    "X = merged_df.drop(columns=['Weekly_Sales', 'Date', 'Holiday_Weight', 'Weighted_Weekly_Sales'])\n",
    "X['Type'] = label_encoder.fit_transform(X['Type'])\n",
    "\n",
    "y = merged_df['Weighted_Weekly_Sales']\n",
    "\n",
    "# Initialize and fit Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=123)\n",
    "\n",
    "output  = cross_validate(rf_model, X, y, cv= 10, scoring = 'r2', return_estimator =True)\n",
    "\n",
    "# Display feature importances for each estimator\n",
    "for idx, estimator in enumerate(output['estimator']):\n",
    "    print(f\"Features sorted by their score for estimator {idx}:\")\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': estimator.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(feature_importances)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Define features and target variable\n",
    "X = merged_df.drop(columns=['Weekly_Sales', 'Date', 'Holiday_Weight', 'Weighted_Weekly_Sales'])\n",
    "X['Type'] = label_encoder.fit_transform(X['Type'])\n",
    "\n",
    "y = merged_df['Weighted_Weekly_Sales']\n",
    "\n",
    "# Initialize and fit Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=123)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Calculate feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances from Random Forest:\")\n",
    "print(feature_importances)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedGroupKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = merged_df.drop(columns=['WeekOfYear','Date', 'Weekly_Sales', 'DayOfWeek'])\n",
    "y = merged_df['Weekly_Sales']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "X['Type'] = label_encoder.fit_transform(X['Type'])\n",
    "X['IsHoliday'] = label_encoder.fit_transform(X['IsHoliday'])\n",
    "\n",
    "linear_regression_model = LinearRegression()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "linear_regression_model.fit(x_train, y_train)\n",
    "y_pred = linear_regression_model.predict(x_test)\n",
    "\n",
    "mse =mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Line of perfect prediction\n",
    "plt.xlabel(\"Actual Weekly Sales\")\n",
    "plt.ylabel(\"Predicted Weekly Sales\")\n",
    "plt.title(\"Actual vs. Predicted Weekly Sales\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(linear_regression_model.coef_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "merged_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedGroupKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = merged_df.drop(columns=['WeekOfYear', 'Date', 'Weighted_Weekly_Sales'])\n",
    "y = merged_df['Weighted_Weekly_Sales']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "X['Type'] = label_encoder.fit_transform(X['Type'])\n",
    "#X['Holiday'] = label_encoder.fit_transform(X['Holiday'])\n",
    "\n",
    "linear_regression_model = LinearRegression()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "linear_regression_model.fit(x_train, y_train)\n",
    "y_pred = linear_regression_model.predict(x_test)\n",
    "\n",
    "mse =mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Line of perfect prediction\n",
    "plt.xlabel(\"Actual Weighted Weekly Sales\")\n",
    "plt.ylabel(\"Predicted Weighted Weekly Sales\")\n",
    "plt.title(\"Actual vs. Predicted Weekly Sales\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(linear_regression_model.coef_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "store_df = merged_df[['Date', 'Weekly_Sales', 'Weighted_Weekly_Sales']]\n",
    "store_df = store_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "unweighted_store_df = store_df[['Date', 'Weekly_Sales']]\n",
    "unweighted_store_df = unweighted_store_df.groupby('Date', as_index=False)['Weekly_Sales'].sum()\n",
    "\n",
    "unweighted_store_df.set_index('Date', inplace=True)\n",
    "\n",
    "unweighted_store_df.head()\n",
    "\n",
    "y = unweighted_store_df['Weekly_Sales']\n",
    "\n",
    "train_size = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "sarima_order = (1, 1, 1)\n",
    "seasonal_order = (1, 1, 1, 52)  # Assuming weekly seasonality (adjust if needed)\n",
    "\n",
    "\n",
    "model = SARIMAX(y_train, order=sarima_order, seasonal_order=seasonal_order)\n",
    "sarima_fit = model.fit(disp=False)\n",
    "\n",
    "\n",
    "forecast = sarima_fit.get_forecast(steps=len(y_test))\n",
    "y_pred = forecast.predicted_mean\n",
    "conf_int = forecast.conf_int()\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "correlation = np.corrcoef(y_test, y_pred)[0, 1]  # Correlation coefficient\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Correlation: {correlation}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(y_train.index, y_train, label='Training Data', color='blue')\n",
    "\n",
    "# Plot actual test data\n",
    "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(y_test.index, y_pred, label='SARIMA Forecast', color='orange')\n",
    "\n",
    "\n",
    "plt.fill_between(y_test.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)\n",
    "\n",
    "plt.title('SARIMA: Actual vs Predicted Weekly Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly Sales')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Add correlation as text to the plot\n",
    "plt.text(x=0.05, y=0.95, s=f\"Correlation: {correlation:.2f}\", fontsize=12,\n",
    "         transform=plt.gca().transAxes, color='darkred', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "store_df = merged_df[['Date', 'Weekly_Sales', 'Weighted_Weekly_Sales']]\n",
    "store_df = store_df.reset_index(drop=True)\n",
    "\n",
    "weighted_store_df = store_df[['Date', 'Weighted_Weekly_Sales']]\n",
    "weighted_store_df = weighted_store_df.groupby('Date', as_index=False)['Weighted_Weekly_Sales'].sum()\n",
    "\n",
    "weighted_store_df.set_index('Date', inplace=True)\n",
    "\n",
    "weighted_store_df.head()\n",
    "\n",
    "y = weighted_store_df['Weighted_Weekly_Sales']\n",
    "\n",
    "train_size = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "sarima_order = (1, 1, 1)\n",
    "seasonal_order = (1, 1, 1, 52)  # Assuming weekly seasonality (adjust if needed)\n",
    "\n",
    "\n",
    "model = SARIMAX(y_train, order=sarima_order, seasonal_order=seasonal_order)\n",
    "sarima_fit = model.fit(disp=False)\n",
    "\n",
    "\n",
    "forecast = sarima_fit.get_forecast(steps=len(y_test))\n",
    "y_pred = forecast.predicted_mean\n",
    "conf_int = forecast.conf_int()\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "correlation = np.corrcoef(y_test, y_pred)[0, 1]  # Correlation coefficient\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Correlation: {correlation}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(y_train.index, y_train, label='Training Data', color='blue')\n",
    "\n",
    "# Plot actual test data\n",
    "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(y_test.index, y_pred, label='SARIMA Forecast', color='orange')\n",
    "\n",
    "\n",
    "plt.fill_between(y_test.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)\n",
    "\n",
    "plt.title('SARIMA: Actual vs Predicted Weighted Weekly Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly Sales')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Add correlation as text to the plot\n",
    "plt.text(x=0.05, y=0.95, s=f\"Correlation: {correlation:.2f}\", fontsize=12,\n",
    "         transform=plt.gca().transAxes, color='darkred', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Test with using multiple variables\n",
    "\n",
    "store_df = merged_df\n",
    "\n",
    "\n",
    "unweighted_store_df = store_df\n",
    "unweighted_store_df.sort_values(by='Date', inplace=True)\n",
    "\n",
    "unweighted_store_df.set_index('Date', inplace=True)\n",
    "\n",
    "unweighted_store_df.head()\n",
    "\n",
    "y = unweighted_store_df['Weekly_Sales']\n",
    "\n",
    "train_size = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "sarima_order = (1, 1, 1)\n",
    "seasonal_order = (1, 1, 1, 52)  # Assuming weekly seasonality (adjust if needed)\n",
    "\n",
    "exog = unweighted_store_df[['Size']]\n",
    "exog_train, exog_test = exog[:train_size], exog[train_size:]\n",
    "\n",
    "\n",
    "model = SARIMAX(y_train, exog=exog_train, order=sarima_order, seasonal_order=seasonal_order)\n",
    "sarima_fit = model.fit(disp=False)\n",
    "\n",
    "\n",
    "\n",
    "forecast = sarima_fit.get_forecast(steps=len(y_test), exog=exog_test)\n",
    "y_pred = forecast.predicted_mean\n",
    "conf_int = forecast.conf_int()\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "correlation = np.corrcoef(y_test, y_pred)[0, 1]  # Correlation coefficient\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Correlation: {correlation}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(y_train.index, y_train, label='Training Data', color='blue')\n",
    "\n",
    "# Plot actual test data\n",
    "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(y_test.index, y_pred, label='SARIMA Forecast', color='orange')\n",
    "\n",
    "\n",
    "plt.fill_between(y_test.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)\n",
    "\n",
    "plt.title('SARIMA: Actual vs Predicted Weekly Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly Sales')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Add correlation as text to the plot\n",
    "plt.text(x=0.05, y=0.95, s=f\"Correlation: {correlation:.2f}\", fontsize=12,\n",
    "         transform=plt.gca().transAxes, color='darkred', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-19T19:00:22.875440Z"
    }
   },
   "source": [
    "# Drop the 'Weighted_Weekly_Sales' column (if applicable) and sort by date\n",
    "new_unweighted_store_df = store_df\n",
    "\n",
    "new_unweighted_store_df.reset_index\n",
    "\n",
    "# Group by 'Date' and apply custom aggregation\n",
    "new_unweighted_store_df = new_unweighted_store_df.groupby('Date', as_index=False).agg({\n",
    "    'Weekly_Sales': 'sum',  # Sum Weekly Sales\n",
    "    'Size': 'first'         # Use the first Size value for each group (or replace with 'mean', 'mode', etc.)\n",
    "})\n",
    "\n",
    "# Sort and set 'Date' as the index\n",
    "new_unweighted_store_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Verify the results\n",
    "print(new_unweighted_store_df.head())\n",
    "\n",
    "# Extract the target variable (y) and exogenous variable (Size)\n",
    "y = new_unweighted_store_df['Weekly_Sales']\n",
    "exog = new_unweighted_store_df[['Size']]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "exog_train, exog_test = exog[:train_size], exog[train_size:]\n",
    "\n",
    "# Fit the SARIMAX model\n",
    "model = SARIMAX(y_train, exog=exog_train, order=sarima_order, seasonal_order=seasonal_order)\n",
    "sarima_fit = model.fit(disp=False)\n",
    "\n",
    "# Forecasting\n",
    "forecast = sarima_fit.get_forecast(steps=len(y_test), exog=exog_test)\n",
    "y_pred = forecast.predicted_mean\n",
    "conf_int = forecast.conf_int()\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "correlation = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Correlation: {correlation}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_train.index, y_train, label='Training Data', color='blue')\n",
    "plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
    "plt.plot(y_test.index, y_pred, label='SARIMA Forecast', color='orange')\n",
    "plt.fill_between(y_test.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)\n",
    "plt.title('SARIMA: Actual vs Predicted Weekly Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly Sales')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.text(\n",
    "    x=0.05, y=0.95, s=f\"Correlation: {correlation:.2f}\", fontsize=12,\n",
    "    transform=plt.gca().transAxes, color='darkred', bbox=dict(facecolor='white', alpha=0.6)\n",
    ")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-19T19:00:23.480377Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
