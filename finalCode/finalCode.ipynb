{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    GridSearchCV, \n",
    "    StratifiedGroupKFold, \n",
    "    cross_validate\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import xgboost as xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Reading CSV files into DataFrames\n",
    "features = pd.read_csv('Features data set.csv')  # Features dataset\n",
    "sales = pd.read_csv('sales data-set.csv')  # Sales dataset\n",
    "stores = pd.read_csv('stores data-set.csv')  # Stores dataset\n",
    "\n",
    "# Dropping the 'IsHoliday' column from the features DataFrame\n",
    "features = features.drop(['IsHoliday'], axis=1)\n",
    "\n",
    "# Merging the sales and features DataFrames on 'Store' and 'Date' columns\n",
    "combined = sales.merge(features, on=['Store', 'Date'])\n",
    "\n",
    "# Merging the combined DataFrame with the stores DataFrame on the 'Store' column\n",
    "combined = combined.merge(stores, on=['Store'])\n",
    "\n",
    "# Filling missing values (NaNs) in the combined DataFrame with 0\n",
    "combined = combined.fillna(value=0)\n",
    "\n",
    "# Grouping the data by several columns and summing up the values for each group\n",
    "merged_df = combined.groupby(['Store', 'Date', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n",
    "                               'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size'], as_index=False).sum()\n",
    "\n",
    "# Dropping the 'Dept' column from the grouped DataFrame\n",
    "merged_df = merged_df.drop(['Dept'], axis = 1)\n",
    "\n",
    "# Rounding all numerical values in the DataFrame to 3 decimal places\n",
    "merged_df = merged_df.round(3)\n",
    "\n",
    "# Encoding the 'Type' column using LabelEncoder (converting categorical values to numerical)\n",
    "label_encoder = LabelEncoder()\n",
    "merged_df['Type'] = label_encoder.fit_transform(merged_df['Type'])\n",
    "\n",
    "# Displaying summary info of the merged DataFrame to understand its structure\n",
    "merged_df.info()\n",
    "\n",
    "# Saving the final merged DataFrame to a CSV file\n",
    "csv_file_path = \"merged_df.csv\"\n",
    "merged_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Printing the first 5 rows of the merged DataFrame to verify the result\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6435 entries, 0 to 6434\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Store         6435 non-null   int64  \n",
      " 1   Date          6435 non-null   object \n",
      " 2   IsHoliday     6435 non-null   bool   \n",
      " 3   Temperature   6435 non-null   float64\n",
      " 4   Fuel_Price    6435 non-null   float64\n",
      " 5   MarkDown1     6435 non-null   float64\n",
      " 6   MarkDown2     6435 non-null   float64\n",
      " 7   MarkDown3     6435 non-null   float64\n",
      " 8   MarkDown4     6435 non-null   float64\n",
      " 9   MarkDown5     6435 non-null   float64\n",
      " 10  CPI           6435 non-null   float64\n",
      " 11  Unemployment  6435 non-null   float64\n",
      " 12  Type          6435 non-null   int32  \n",
      " 13  Size          6435 non-null   int64  \n",
      " 14  Weekly_Sales  6435 non-null   float64\n",
      "dtypes: bool(1), float64(10), int32(1), int64(2), object(1)\n",
      "memory usage: 685.1+ KB\n",
      "   Store        Date  IsHoliday  Temperature  Fuel_Price  MarkDown1  \\\n",
      "0      1  01/04/2011      False        59.17       3.524       0.00   \n",
      "1      1  01/06/2012      False        77.95       3.501    6086.21   \n",
      "2      1  01/07/2011      False        85.55       3.524       0.00   \n",
      "3      1  01/10/2010      False        71.89       2.603       0.00   \n",
      "4      1  02/03/2012      False        60.96       3.630   15441.40   \n",
      "\n",
      "   MarkDown2  MarkDown3  MarkDown4  MarkDown5      CPI  Unemployment  Type  \\\n",
      "0        0.0       0.00       0.00       0.00  214.837         7.682     0   \n",
      "1       12.0     370.51     148.75    3690.85  221.747         7.143     0   \n",
      "2        0.0       0.00       0.00       0.00  215.184         7.962     0   \n",
      "3        0.0       0.00       0.00       0.00  211.672         7.838     0   \n",
      "4     1569.0      10.80   25390.88    8067.61  220.848         7.348     0   \n",
      "\n",
      "     Size  Weekly_Sales  \n",
      "0  151315    1495064.75  \n",
      "1  151315    1624477.58  \n",
      "2  151315    1488538.09  \n",
      "3  151315    1453329.50  \n",
      "4  151315    1688420.76  \n"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('Features data set.csv')\n",
    "sales = pd.read_csv('sales data-set.csv')\n",
    "stores = pd.read_csv('stores data-set.csv')\n",
    "\n",
    "features = features.drop(['IsHoliday'], axis=1)\n",
    "combined = sales.merge(features, on=['Store', 'Date'])\n",
    "combined = combined.merge(stores, on=['Store'])\n",
    "\n",
    "combined = combined.fillna(value=0)\n",
    "\n",
    "merged_df = combined.groupby(['Store', 'Date', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n",
    "                               'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size'], as_index=False).sum()\n",
    "\n",
    "merged_df = merged_df.drop(['Dept'], axis = 1)\n",
    "merged_df = merged_df.round(3)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "merged_df['Type'] = label_encoder.fit_transform(merged_df['Type'])\n",
    "merged_df.info()\n",
    "\n",
    "csv_file_path = \"merged_df.csv\"\n",
    "merged_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'Date' column to datetime format with the day-first option (e.g., DD/MM/YYYY)\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'], dayfirst=True)\n",
    "\n",
    "# Extracting the 'Year' from the 'Date' column and creating a new 'Year' column\n",
    "merged_df['Year'] = merged_df['Date'].dt.year\n",
    "\n",
    "# Extracting the 'Month' from the 'Date' column and creating a new 'Month' column\n",
    "merged_df['Month'] = merged_df['Date'].dt.month\n",
    "\n",
    "# Extracting the 'WeekOfYear' from the 'Date' column and creating a new 'WeekOfYear' column\n",
    "merged_df['WeekOfYear'] = merged_df['Date'].dt.isocalendar().week\n",
    "\n",
    "# Converting 'WeekOfYear' to an integer type for more efficient memory usage\n",
    "merged_df['WeekOfYear'] = merged_df['WeekOfYear'].astype(np.int32)\n",
    "\n",
    "# Extracting the 'DayOfWeek' from the 'Date' column (0 = Monday, 6 = Sunday) and creating a new 'DayOfWeek' column\n",
    "merged_df['DayOfWeek'] = merged_df['Date'].dt.dayofweek\n",
    "\n",
    "# Sorting the DataFrame first by 'Store' and then by 'Date' to maintain a consistent order\n",
    "merged_df = merged_df.sort_values(by=['Store', 'Date'])\n",
    "\n",
    "# Initializing a new column 'Holiday_Weight' with a default value of 1 for all rows\n",
    "merged_df['Holiday_Weight'] = 1\n",
    "\n",
    "# Identifying the Super Bowl week: February, with week <= 6, and a Saturday (dayofweek == 6)\n",
    "super_bowl_week = (merged_df['Date'].dt.month == 2) & (merged_df['Date'].dt.isocalendar().week <= 6) & (merged_df['Date'].dt.dayofweek == 6)\n",
    "\n",
    "# Identifying Labor Day week: September, and the 36th week of the year\n",
    "labor_day_week = (merged_df['Date'].dt.month == 9) & (merged_df['Date'].dt.isocalendar().week == 36)\n",
    "\n",
    "# Identifying Thanksgiving season: Weeks 47 and 48 of the year (significant sales spike observed)\n",
    "thanksgiving_week = merged_df['WeekOfYear'].isin([47, 48])\n",
    "\n",
    "# Identifying the Christmas season: Weeks 49, 50, and 51 of the year (another spike in sales)\n",
    "christmas_period = merged_df['WeekOfYear'].isin([49, 50, 51])\n",
    "\n",
    "# Assigning a weight of 5 to rows that correspond to the identified holiday periods\n",
    "merged_df.loc[super_bowl_week, 'Holiday_Weight'] = 5\n",
    "merged_df.loc[labor_day_week, 'Holiday_Weight'] = 5\n",
    "merged_df.loc[thanksgiving_week, 'Holiday_Weight'] = 5\n",
    "merged_df.loc[christmas_period, 'Holiday_Weight'] = 5\n",
    "\n",
    "# Creating a new column 'Weighted_Weekly_Sales' which adjusts 'Weekly_Sales' based on the 'Holiday_Weight'\n",
    "merged_df['Weighted_Weekly_Sales'] = merged_df['Weekly_Sales'] * merged_df['Holiday_Weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'], dayfirst=True)\n",
    "merged_df['Year'] = merged_df['Date'].dt.year\n",
    "merged_df['Month'] = merged_df['Date'].dt.month\n",
    "\n",
    "merged_df['WeekOfYear'] = merged_df['Date'].dt.isocalendar().week\n",
    "merged_df['WeekOfYear'] = merged_df['WeekOfYear'].astype(np.int32)\n",
    "\n",
    "merged_df['DayOfWeek'] = merged_df['Date'].dt.dayofweek\n",
    "\n",
    "merged_df = merged_df.sort_values(by=['Store', 'Date'])\n",
    "\n",
    "# Initialize the Holiday_Weight column with a default value of 1\n",
    "merged_df['Holiday_Weight'] = 1\n",
    "\n",
    "# Identify specific holiday periods based on the observed data\n",
    "super_bowl_week = (merged_df['Date'].dt.month == 2) & (merged_df['Date'].dt.isocalendar().week <= 6) & (merged_df['Date'].dt.dayofweek == 6)\n",
    "labor_day_week = (merged_df['Date'].dt.month == 9) & (merged_df['Date'].dt.isocalendar().week == 36)\n",
    "\n",
    "# Thanksgiving season: Weeks 47 and 48 (spike observed)\n",
    "thanksgiving_week = merged_df['WeekOfYear'].isin([47, 48])\n",
    "\n",
    "# Christmas season: Weeks 49, 50, and 51 (significant spike observed)\n",
    "christmas_period = merged_df['WeekOfYear'].isin([49, 50, 51])\n",
    "\n",
    "# Apply weights to the Holiday_Weight column\n",
    "merged_df.loc[super_bowl_week, 'Holiday_Weight'] = 5\n",
    "merged_df.loc[labor_day_week, 'Holiday_Weight'] = 5\n",
    "merged_df.loc[thanksgiving_week, 'Holiday_Weight'] = 5\n",
    "merged_df.loc[christmas_period, 'Holiday_Weight'] = 5\n",
    "\n",
    "# Create a new column for weighted weekly sales\n",
    "merged_df['Weighted_Weekly_Sales'] = merged_df['Weekly_Sales'] * merged_df['Holiday_Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numerical columns from the merged DataFrame\n",
    "# Drop 'Weekly_Sales', 'Weighted_Weekly_Sales', and 'Holiday_Weight' columns as they are target variables or already used for computation\n",
    "numeric_df = merged_df.select_dtypes(include=[np.number]).drop(columns=['Weekly_Sales', 'Weighted_Weekly_Sales', 'Holiday_Weight'])\n",
    "\n",
    "# Compute the correlation of each numerical feature with the 'Weekly_Sales' column\n",
    "# .corrwith() computes the correlation of each column in numeric_df with 'Weekly_Sales'\n",
    "correlation = numeric_df.corrwith(merged_df['Weekly_Sales']).sort_values(ascending=False)\n",
    "\n",
    "# Print the correlation of each feature with 'Weekly_Sales'\n",
    "print(\"Correlation of each feature with Weekly_Sales:\")\n",
    "print(correlation)\n",
    "\n",
    "# Create a heatmap to visualize the correlation values\n",
    "plt.figure(figsize=(10, 6))  # Set the size of the figure for better readability\n",
    "\n",
    "# Use seaborn's heatmap function to display the correlation values as a color map\n",
    "# Convert the correlation series to a DataFrame for heatmap compatibility\n",
    "sns.heatmap(correlation.to_frame(), annot=True, cmap=\"coolwarm\", cbar=True, fmt=\".2f\")\n",
    "\n",
    "# Set the title of the heatmap plot\n",
    "plt.title(\"Correlation with Weighted Weekly Sales\")\n",
    "\n",
    "# Label the x-axis\n",
    "plt.xlabel(\"Weighted Weekly Sales\")\n",
    "\n",
    "# Label the y-axis with feature names\n",
    "plt.ylabel(\"Features\")\n",
    "\n",
    "# Adjust the layout to avoid overlap and make the plot look clean\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the heatmap plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = merged_df.select_dtypes(include=[np.number]).drop(columns=['Weekly_Sales', 'Weighted_Weekly_Sales', 'Holiday_Weight', ])\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation = numeric_df.corrwith(merged_df['Weekly_Sales']).sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlation of each feature with Weighted_Weekly_Sales:\")\n",
    "print(correlation)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.heatmap(correlation.to_frame(), annot=True, cmap=\"coolwarm\", cbar=True, fmt=\".2f\")\n",
    "\n",
    "plt.title(\"Correlation with Weighted Weekly Sales\")\n",
    "\n",
    "plt.xlabel(\"Weighted Weekly Sales\")\n",
    "\n",
    "plt.ylabel(\"Features\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initializing the PCA and StandardScaler objects\n",
    "pca = PCA()  # Principal Component Analysis (PCA) object to perform dimensionality reduction\n",
    "scaler = StandardScaler()  # StandardScaler object to scale/standardize the data\n",
    "\n",
    "# Making a copy of the merged DataFrame to preserve the original unscaled data\n",
    "combined_scaled = merged_df.copy()\n",
    "\n",
    "# Scaling each numerical column individually to normalize the data\n",
    "# This step is important because PCA is sensitive to the scale of the data\n",
    "\n",
    "combined_scaled['Weekly_Sales'] = scaler.fit_transform(combined_scaled[['Weekly_Sales']])\n",
    "combined_scaled['Temperature'] = scaler.fit_transform(combined_scaled[['Temperature']])\n",
    "combined_scaled['Fuel_Price'] = scaler.fit_transform(combined_scaled[['Fuel_Price']])\n",
    "combined_scaled['MarkDown1'] = scaler.fit_transform(combined_scaled[['MarkDown1']])\n",
    "combined_scaled['MarkDown2'] = scaler.fit_transform(combined_scaled[['MarkDown2']])\n",
    "combined_scaled['MarkDown3'] = scaler.fit_transform(combined_scaled[['MarkDown3']])\n",
    "combined_scaled['MarkDown4'] = scaler.fit_transform(combined_scaled[['MarkDown4']])\n",
    "combined_scaled['MarkDown5'] = scaler.fit_transform(combined_scaled[['MarkDown5']])\n",
    "combined_scaled['CPI'] = scaler.fit_transform(combined_scaled[['CPI']])\n",
    "combined_scaled['Unemployment'] = scaler.fit_transform(combined_scaled[['Unemployment']])\n",
    "combined_scaled['Size'] = scaler.fit_transform(combined_scaled[['Size']])\n",
    "combined_scaled['Weighted_Weekly_Sales'] = scaler.fit_transform(combined_scaled[['Weighted_Weekly_Sales']])\n",
    "combined_scaled['Type'] = scaler.fit_transform(combined_scaled[['Type']])\n",
    "\n",
    "# Display the first few rows of the scaled DataFrame\n",
    "combined_scaled.head()\n",
    "\n",
    "# Performing PCA on the scaled numerical features\n",
    "# Here, the features selected for PCA are adjusted, in this case, removing 'Weekly_Sales' and 'Weighted_Weekly_Sales'\n",
    "# PCA works better when there is no collinearity between the features, so we leave out the target variables\n",
    "\n",
    "# Fit PCA model using the scaled features, excluding 'Weekly_Sales' and 'Weighted_Weekly_Sales'\n",
    "pca.fit(combined_scaled[['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Size', 'Type']])\n",
    "\n",
    "# Output the explained variance ratio for each principal component\n",
    "# This shows how much variance each principal component explains in the data\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Creating an array of Principal Component (PC) numbers starting from 1\n",
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "\n",
    "# Plotting the Scree Plot to visualize the explained variance ratio of each component\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')  # Title of the plot\n",
    "plt.xlabel('Principal Component')  # Label for the x-axis (Principal Components)\n",
    "plt.ylabel('Variance Explained')  # Label for the y-axis (Explained Variance)\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#making a copy to preserve the original unscaled df\n",
    "combined_scaled = merged_df.copy()\n",
    "\n",
    "#scaling each of the numerical columns\n",
    "combined_scaled['Weekly_Sales'] = scaler.fit_transform(combined_scaled[['Weekly_Sales']])\n",
    "combined_scaled['Temperature'] = scaler.fit_transform(combined_scaled[['Temperature']])\n",
    "combined_scaled['Fuel_Price'] = scaler.fit_transform(combined_scaled[['Fuel_Price']])\n",
    "combined_scaled['MarkDown1'] = scaler.fit_transform(combined_scaled[['MarkDown1']])\n",
    "combined_scaled['MarkDown2'] = scaler.fit_transform(combined_scaled[['MarkDown2']])\n",
    "combined_scaled['MarkDown3'] = scaler.fit_transform(combined_scaled[['MarkDown3']])\n",
    "combined_scaled['MarkDown4'] = scaler.fit_transform(combined_scaled[['MarkDown4']])\n",
    "combined_scaled['MarkDown5'] = scaler.fit_transform(combined_scaled[['MarkDown5']])\n",
    "combined_scaled['CPI'] = scaler.fit_transform(combined_scaled[['CPI']])\n",
    "combined_scaled['Unemployment'] = scaler.fit_transform(combined_scaled[['Unemployment']])\n",
    "combined_scaled['Size'] = scaler.fit_transform(combined_scaled[['Size']])\n",
    "combined_scaled['Weighted_Weekly_Sales'] = scaler.fit_transform(combined_scaled[['Weighted_Weekly_Sales']])\n",
    "combined_scaled['Type'] = scaler.fit_transform(combined_scaled[['Type']])\n",
    "combined_scaled.head()\n",
    "\n",
    "#performing PCA on the numerical features. Which features we select will likely need tweaking\n",
    "#pca.fit(combined_scaled[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Size', 'Weighted_Weekly_Sales']])\n",
    "\n",
    "pca.fit(combined_scaled[['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Size', 'Type']])\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = merged_df.drop(columns=['Weekly_Sales', 'Date', 'Weighted_Weekly_Sales'])\n",
    "X['Type'] = label_encoder.fit_transform(X['Type'])\n",
    "\n",
    "y = merged_df['Weighted_Weekly_Sales']\n",
    "\n",
    "# Initialize and fit Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=123)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Calculate feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances from Random Forest:\")\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target (y)\n",
    "# Drop 'Date', 'Weekly_Sales', 'DayOfWeek', and 'Weighted_Weekly_Sales' from the features (X)\n",
    "# 'Weekly_Sales' is the target variable, so it is separated as 'y'\n",
    "X = merged_df.drop(columns=['Date', 'Weekly_Sales', 'DayOfWeek', 'Weighted_Weekly_Sales'])\n",
    "y = merged_df['Weekly_Sales']\n",
    "\n",
    "# Initialize a LabelEncoder object to convert categorical variables into numeric form\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding on the 'Type' and 'IsHoliday' columns to convert them into numeric labels\n",
    "X['Type'] = label_encoder.fit_transform(X['Type'])\n",
    "X['IsHoliday'] = label_encoder.fit_transform(X['IsHoliday'])\n",
    "\n",
    "# Initialize a Linear Regression model\n",
    "linear_regression_model = LinearRegression()\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "# train_test_split ensures that the model is evaluated on unseen data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the linear regression model to the training data\n",
    "linear_regression_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict the target values (Weekly_Sales) using the test data\n",
    "y_pred = linear_regression_model.predict(x_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and R-squared (R2) for model performance evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the model's Mean Squared Error and R-squared values\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Create a scatter plot comparing actual vs. predicted Weekly Sales\n",
    "# 'alpha=0.5' makes the points semi-transparent to avoid overlap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "\n",
    "# Plot a red dashed line representing the line of perfect prediction (where actual = predicted)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel(\"Actual Weekly Sales\")\n",
    "plt.ylabel(\"Predicted Weekly Sales\")\n",
    "plt.title(\"Actual vs. Predicted Weekly Sales\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the coefficients of the linear regression model (importance of each feature)\n",
    "print(linear_regression_model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df.drop(columns=['Date','Weekly_Sales', 'DayOfWeek', 'Weighted_Weekly_Sales'])\n",
    "y = merged_df['Weekly_Sales']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "X['Type'] = label_encoder.fit_transform(X['Type'])\n",
    "X['IsHoliday'] = label_encoder.fit_transform(X['IsHoliday'])\n",
    "\n",
    "linear_regression_model = LinearRegression()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "linear_regression_model.fit(x_train, y_train)\n",
    "y_pred = linear_regression_model.predict(x_test)\n",
    "\n",
    "mse =mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Line of perfect prediction\n",
    "plt.xlabel(\"Actual Weekly Sales\")\n",
    "plt.ylabel(\"Predicted Weekly Sales\")\n",
    "plt.title(\"Actual vs. Predicted Weekly Sales\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(linear_regression_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Predict weekly sales for 2013\n",
    "output_dir = 'linearRegressionStorePredictions'\n",
    "os.makedirs(output_dir, exist_ok=True) \n",
    "\n",
    "# Load the synthetic adjusted 2013 dataset\n",
    "synthetic_2013_file_path = 'synthetic_holiday_weekly_2013_adjusted_data.csv'\n",
    "synthetic_2013_df = pd.read_csv(synthetic_2013_file_path)\n",
    "\n",
    "# Output directory for predictions\n",
    "output_dir = 'linearRegressionStorePredictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format\n",
    "synthetic_2013_df['Date'] = pd.to_datetime(synthetic_2013_df['Date'])\n",
    "\n",
    "# Prepare feature columns for prediction, excluding target columns\n",
    "feature_columns = synthetic_2013_df.columns.drop(['Date', 'Weekly_Sales', 'DayOfWeek', 'Weighted_Weekly_Sales'])\n",
    "\n",
    "# Ensure features like `Type` and `IsHoliday` are encoded properly\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "synthetic_2013_df['Type'] = label_encoder.fit_transform(synthetic_2013_df['Type'])\n",
    "synthetic_2013_df['IsHoliday'] = label_encoder.fit_transform(synthetic_2013_df['IsHoliday'])\n",
    "\n",
    "# Sanitize input data\n",
    "def sanitize_input(input_df):\n",
    "    input_df = input_df.replace([float('inf'), -float('inf')], float('nan'))  # Replace infinities with NaN\n",
    "    input_df = input_df.fillna(0)  # Replace NaN with 0\n",
    "    return input_df\n",
    "\n",
    "# Prepare the sanitized feature set\n",
    "X = sanitize_input(synthetic_2013_df[feature_columns])\n",
    "\n",
    "# Predict weekly sales using the linear regression model\n",
    "predicted_sales = linear_regression_model.predict(X)\n",
    "\n",
    "# Combine predictions with dates for output\n",
    "predictions_2013 = pd.DataFrame({\n",
    "    'Date': synthetic_2013_df['Date'],\n",
    "    'Store': synthetic_2013_df['Store'],\n",
    "    'Predicted_Weekly_Sales': predicted_sales\n",
    "})\n",
    "\n",
    "predictions_2013['Predicted_Weekly_Sales'] = predictions_2013['Predicted_Weekly_Sales'].round(3)\n",
    "\n",
    "# Save predictions to CSV\n",
    "output_file = os.path.join(output_dir, 'predicted_sales_2013.csv')\n",
    "predictions_2013.to_csv(output_file, index=False)\n",
    "\n",
    "# Print completion message and show the first few rows\n",
    "print(f\"Predictions saved to {output_file}\")\n",
    "print(predictions_2013.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Linear Regression and Lasso Regression\n",
    "X_lr_lasso = merged_df.drop(columns=['Date', 'Weekly_Sales', 'DayOfWeek', 'Weighted_Weekly_Sales'])\n",
    "y_lr_lasso = merged_df['Weekly_Sales']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "X_lr_lasso['Type'] = label_encoder.fit_transform(X_lr_lasso['Type'])\n",
    "X_lr_lasso['IsHoliday'] = label_encoder.fit_transform(X_lr_lasso['IsHoliday'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_lr_lasso_scaled = scaler.fit_transform(X_lr_lasso)\n",
    "\n",
    "# Splitting the data\n",
    "x_train_lr_lasso, x_test_lr_lasso, y_train_lr_lasso, y_test_lr_lasso = train_test_split(\n",
    "    X_lr_lasso, y_lr_lasso, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "### Linear Regression\n",
    "linear_regression_model = LinearRegression()\n",
    "linear_regression_model.fit(x_train_lr_lasso, y_train_lr_lasso)\n",
    "y_pred_lr = linear_regression_model.predict(x_test_lr_lasso)\n",
    "\n",
    "mse_lr = mean_squared_error(y_test_lr_lasso, y_pred_lr)\n",
    "r2_lr = r2_score(y_test_lr_lasso, y_pred_lr)\n",
    "\n",
    "print(\"Linear Regression\")\n",
    "print(f\"Mean Squared Error: {mse_lr}\")\n",
    "print(f\"R-squared: {r2_lr}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_lr_lasso, y_pred_lr, alpha=0.5)\n",
    "plt.plot([y_test_lr_lasso.min(), y_test_lr_lasso.max()], [y_test_lr_lasso.min(), y_test_lr_lasso.max()], 'r--')\n",
    "plt.xlabel(\"Actual Weekly Sales\")\n",
    "plt.ylabel(\"Predicted Weekly Sales\")\n",
    "plt.title(\"Linear Regression: Actual vs. Predicted Weekly Sales\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "### Lasso Regression with Grid Search\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(Lasso(max_iter=10000), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(scaler.fit_transform(x_train_lr_lasso), y_train_lr_lasso)\n",
    "\n",
    "# Retrieve the best alpha\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(f\"Best alpha for Lasso: {best_alpha}\")\n",
    "\n",
    "# Train Lasso model with the best alpha\n",
    "lasso = Lasso(alpha=best_alpha, max_iter=10000)\n",
    "lasso.fit(scaler.fit_transform(x_train_lr_lasso), y_train_lr_lasso)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lasso = lasso.predict(scaler.transform(x_test_lr_lasso))\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lasso = mean_squared_error(y_test_lr_lasso, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test_lr_lasso, y_pred_lasso)\n",
    "\n",
    "print(\"Lasso Regression\")\n",
    "print(f\"Mean Squared Error: {mse_lasso:.2f}\")\n",
    "print(f\"R-squared: {r2_lasso:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_lr_lasso, y_pred_lasso, alpha=0.5, label=\"Predicted\")\n",
    "plt.plot([y_test_lr_lasso.min(), y_test_lr_lasso.max()], [y_test_lr_lasso.min(), y_test_lr_lasso.max()], 'r--', label=\"Perfect Prediction Line\")\n",
    "plt.xlabel(\"Actual Weekly Sales\")\n",
    "plt.ylabel(\"Predicted Weekly Sales\")\n",
    "plt.title(\"Lasso Regression: Actual vs. Predicted Weekly Sales\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = merged_df.drop(columns=['Weekly_Sales','Date', 'DayOfWeek', 'Weighted_Weekly_Sales'])  # Excluding target and irrelevant columns\n",
    "y = merged_df['Weekly_Sales']\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "if 'Type' in X.columns:\n",
    "    X['Type'] = label_encoder.fit_transform(X['Type'])\n",
    "if 'IsHoliday' in X.columns:\n",
    "    X['IsHoliday'] = label_encoder.fit_transform(X['IsHoliday'])\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Regressor\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "random_forest_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = random_forest_model.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Regressor\")\n",
    "print(f\"Mean Squared Error: {mse_rf:.2f}\")\n",
    "print(f\"R-squared: {r2_rf:.2f}\")\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_rf, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label=\"Perfect Prediction Line\")\n",
    "plt.xlabel(\"Actual Weekly Sales\")\n",
    "plt.ylabel(\"Predicted Weekly Sales\")\n",
    "plt.title(\"Random Forest: Actual vs. Predicted Weekly Sales\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importances = random_forest_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "results_df = x_test.copy()\n",
    "results_df['Actual Sales'] = y_test.values\n",
    "results_df['Predicted Sales'] = y_pred_rf\n",
    "results_df['Date'] = merged_df.loc[y_test.index, 'Date']  # Retrieve corresponding dates\n",
    "results_df.sort_values(by='Date', inplace=True)  # Sort by date\n",
    "\n",
    "# Plot actual vs predicted sales over time\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(results_df['Date'], results_df['Actual Sales'], label=\"Actual Sales\", alpha=0.7, linewidth=2)\n",
    "plt.plot(results_df['Date'], results_df['Predicted Sales'], label=\"Predicted Sales\", alpha=0.7, linewidth=2)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.title(\"Random Forest: Actual vs Predicted Sales Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xg\n",
    "\n",
    "# Prepare the feature set (X) and the target variable (y) for training and testing\n",
    "# X includes relevant features for predicting weekly sales (excluding 'Weekly_Sales')\n",
    "# y is the target variable, i.e., 'Weekly_Sales'\n",
    "X, y = merged_df[['Date', 'Store', 'Month', 'WeekOfYear', 'DayOfWeek', 'IsHoliday', \n",
    "                  'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', \n",
    "                  'MarkDown4', 'MarkDown5', 'Size']], merged_df[['Date', 'Weekly_Sales']]\n",
    "\n",
    "# Set the 'Date' column as the index for both X and y DataFrames for easy time-based operations\n",
    "X = X.set_index('Date')\n",
    "y = y.set_index('Date')\n",
    "\n",
    "# Split data into training and testing sets based on the years\n",
    "# Training data will contain all data from 2010 and 2011\n",
    "# Testing data will contain data from 2012\n",
    "x_train = pd.concat([X.loc['2010'], X.loc['2011']])  # Concatenate data for 2010 and 2011\n",
    "y_train = pd.concat([y.loc['2010'], y.loc['2011']])  # Concatenate target sales for 2010 and 2011\n",
    "x_test = X.loc['2012']  # Test data for 2012\n",
    "y_test = y.loc['2012']  # Actual sales for 2012\n",
    "\n",
    "# Output the training data (optional check to view data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = merged_df[['Date', 'Store', 'Month', 'WeekOfYear', 'DayOfWeek', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Size']], merged_df[['Date', 'Weekly_Sales']]\n",
    "\n",
    "X = X.set_index('Date')\n",
    "y = y.set_index('Date')\n",
    "x_train = pd.concat([X.loc['2010'], X.loc['2011']])\n",
    "y_train = pd.concat([y.loc['2010'], y.loc['2011']])\n",
    "x_test = X.loc['2012']\n",
    "y_test = y.loc['2012']\n",
    "print(x_train)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "xg_regressor = xg.XGBRegressor(objective='reg:linear', device='cuda')\n",
    "\n",
    "#cv_scores = cross_val_score(xg_regressor, X, y, cv=10)\n",
    "xg_regressor.fit(x_train, y_train)\n",
    "y_pred = xg_regressor.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_pred = pd.DataFrame({'Date': y_test.index,'Predicted': y_pred})\n",
    "y_pred = y_pred.set_index('Date')\n",
    "\n",
    "y_test_sums = y_test.groupby(y_test.index).sum()\n",
    "y_pred_sums = y_pred.groupby(y_pred.index).sum()\n",
    "\n",
    "#print(\"Cross Validation scores:\", cv_scores)\n",
    "#print(\"Mean cross-validation score:\", cv_scores.mean())\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2:\", r2)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_test_sums, label='Actual Sales')\n",
    "plt.plot(y_pred_sums, label='Predicted Sales')\n",
    "plt.title(\"XGBoost Actual vs Predicted Line Plot\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for DBSCAN\n",
    "X_dbscan = merged_df[['Weekly_Sales', 'MarkDown3','MarkDown1', 'MarkDown4']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_dbscan_scaled = scaler.fit_transform(X_dbscan)\n",
    "\n",
    "# Perform DBSCAN clustering\n",
    "eps_values = [0.5, 1.0, 1.5, 2.0]  # Try different epsilon values\n",
    "min_samples = 5  # Minimum number of points in a neighborhood to form a cluster\n",
    "best_silhouette_score = -1  # Initialize the best silhouette score\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(X_dbscan_scaled)\n",
    "    \n",
    "    # Silhouette Score (only calculate if there are more than 1 cluster)\n",
    "    if len(set(labels)) > 1:\n",
    "        silhouette_avg = silhouette_score(X_dbscan_scaled, labels)\n",
    "        print(f\"DBSCAN with eps={eps}: Silhouette Score = {silhouette_avg:.3f}\")\n",
    "        \n",
    "        if silhouette_avg > best_silhouette_score:\n",
    "            best_silhouette_score = silhouette_avg\n",
    "            best_eps = eps\n",
    "            best_labels = labels\n",
    "    else:\n",
    "        print(f\"DBSCAN with eps={eps}: Only one cluster or noise points detected.\")\n",
    "\n",
    "# Output the best epsilon value\n",
    "print(f\"\\nBest eps value based on Silhouette Score: {best_eps}\")\n",
    "print(f\"Best Silhouette Score: {best_silhouette_score:.3f}\")\n",
    "\n",
    "# Final DBSCAN with the best eps\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=min_samples)\n",
    "final_labels = dbscan.fit_predict(X_dbscan_scaled)\n",
    "\n",
    "# Add the cluster assignments to the original dataset\n",
    "X_dbscan['Cluster'] = final_labels\n",
    "\n",
    "# Print the number of clusters and noise points\n",
    "n_clusters = len(set(final_labels)) - (1 if -1 in final_labels else 0)  # Exclude noise points (-1 label)\n",
    "n_noise = list(final_labels).count(-1)\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_dbscan['MarkDown3'], X_dbscan['Weekly_Sales'], c=final_labels, cmap='viridis', alpha=0.5)\n",
    "plt.xlabel(\"MarkDown3\")\n",
    "plt.ylabel(\"Weekly Sales\")\n",
    "plt.title(f\"DBSCAN Clustering with eps={best_eps}\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create an output directory to save the forecasts if it does not exist\n",
    "output_dir = 'storePredictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Extract relevant columns from the merged dataset for store-level analysis\n",
    "store_df = merged_df[['Date', 'Store', 'Weekly_Sales', 'Weighted_Weekly_Sales']]\n",
    "\n",
    "# Reset the index to avoid using it in the DataFrame\n",
    "store_df = store_df.reset_index(drop=True)\n",
    "\n",
    "# Create a new DataFrame containing only the date, store, and weekly sales (without weighting)\n",
    "unweighted_store_df = store_df[['Date', 'Store', 'Weekly_Sales']]\n",
    "\n",
    "# Check unique store IDs to ensure we iterate through each store individually\n",
    "print(unweighted_store_df['Store'].unique())\n",
    "\n",
    "# Loop through each store in the unweighted DataFrame and apply time series forecasting\n",
    "for store_id in unweighted_store_df['Store'].unique():\n",
    "\n",
    "    # Filter data for the current store\n",
    "    unweighted_df = unweighted_store_df.loc[unweighted_store_df['Store'] == store_id]\n",
    "\n",
    "    # Set 'Date' as the index for time series forecasting\n",
    "    unweighted_df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Define the target variable (weekly sales)\n",
    "    y = unweighted_df['Weekly_Sales']\n",
    "\n",
    "    # Split data into training and test sets (80% for training, 20% for testing)\n",
    "    train_size = int(len(y) * 0.8)\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    # Define SARIMA model orders\n",
    "    sarima_order = (1, 1, 1)  # (p, d, q) - Non-seasonal order\n",
    "    seasonal_order = (1, 1, 1, 52)  # (P, D, Q, S) - Seasonal order, 52 weeks for yearly seasonality\n",
    "\n",
    "    # Initialize and fit the SARIMA model to the training data\n",
    "    model = SARIMAX(y_train, order=sarima_order, seasonal_order=seasonal_order)\n",
    "    sarima_fit = model.fit(disp=False)\n",
    "\n",
    "    # Make predictions for the test set\n",
    "    forecast = sarima_fit.get_forecast(steps=len(y_test))\n",
    "    y_pred = forecast.predicted_mean  # Predicted weekly sales\n",
    "    conf_int = forecast.conf_int()  # Confidence intervals for predictions\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    mse = mean_squared_error(y_test, y_pred)  # Mean squared error of the predictions\n",
    "    correlation = np.corrcoef(y_test, y_pred)[0, 1]  # Correlation coefficient between actual and predicted values\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Correlation: {correlation}\")\n",
    "\n",
    "    # Plot the actual vs predicted weekly sales for the store\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot training data (blue)\n",
    "    plt.plot(y_train.index, y_train, label='Training Data', color='blue')\n",
    "\n",
    "    # Plot actual test data (green)\n",
    "    plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
    "\n",
    "    # Plot predicted sales (orange)\n",
    "    plt.plot(y_test.index, y_pred, label='SARIMA Forecast', color='orange')\n",
    "\n",
    "    # Plot the confidence intervals (pink shade)\n",
    "    plt.fill_between(y_test.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)\n",
    "\n",
    "    # Add title and labels to the plot\n",
    "    plt.title(f'SARIMA: Actual vs Predicted Weekly Sales for Store {store_id}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Weekly Sales')\n",
    "    plt.legend()  # Show the legend for each plot line\n",
    "    plt.grid(True)  # Show grid lines for better readability\n",
    "\n",
    "    # Add the correlation value as text on the plot\n",
    "    plt.text(x=0.05, y=0.95, s=f\"Correlation: {correlation:.2f}\", fontsize=12,\n",
    "             transform=plt.gca().transAxes, color='darkred', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "    # Save the plot as a PNG file with a unique filename for each store\n",
    "    filename = os.path.join(output_dir, f'Store_{store_id}_Forecasting.png')\n",
    "    plt.savefig(filename)  # Save the plot\n",
    "    plt.close()  # Close the plot to free up memory for the next iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Create an output directory to save the forecasted plots if it does not exist\n",
    "output_dir = 'TESForcastingPredictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Extract relevant columns for store-level analysis: Date, Store, Weekly Sales, and Weighted Weekly Sales\n",
    "store_df = merged_df[['Date', 'Store', 'Weekly_Sales', 'Weighted_Weekly_Sales']]\n",
    "\n",
    "# Loop through each store to generate forecasts\n",
    "for store_id in store_df['Store'].unique():\n",
    "\n",
    "    # Filter the data for the current store\n",
    "    unweighted_store_df = store_df.loc[store_df['Store'] == store_id]\n",
    "    \n",
    "    # Set 'Date' as the index for time series forecasting\n",
    "    unweighted_store_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Define and fit the Exponential Smoothing model to the store's weekly sales data\n",
    "    model = ExponentialSmoothing(\n",
    "        unweighted_store_df['Weekly_Sales'],  # Target variable (weekly sales)\n",
    "        seasonal='mul',  # Specify multiplicative seasonality (adjust based on your data's pattern)\n",
    "        seasonal_periods=52,  # Set the seasonal period to 52 for weekly seasonality (1 year)\n",
    "        trend=None  # No trend component in the model\n",
    "    )\n",
    "    fit = model.fit()  # Fit the model to the data\n",
    "    \n",
    "    # Forecast the next 52 weeks (one year)\n",
    "    forecast = fit.forecast(52)\n",
    "    \n",
    "    # Plot the original weekly sales and the forecasted values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original weekly sales data\n",
    "    plt.plot(unweighted_store_df['Weekly_Sales'], label='Original Data')\n",
    "    \n",
    "    # Plot forecasted sales for the next 52 weeks\n",
    "    plt.plot(forecast.index, forecast, label='Forecast', color='orange')\n",
    "    \n",
    "    # Add title and labels to the plot\n",
    "    plt.title(f'Weekly Sales and Forecast for Store {store_id}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Weekly Sales')\n",
    "    plt.legend()  # Display legend to differentiate between the original and forecasted data\n",
    "    \n",
    "    # Save the forecast plot for the current store as a PNG image\n",
    "    filename = os.path.join(output_dir, f'Store_{store_id}_Forecasting.png')\n",
    "    plt.savefig(filename)  # Save the figure to the specified filename\n",
    "    plt.close()  # Close the plot to free up memory before moving to the next store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'storePredictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "store_df = merged_df[['Date', 'Store', 'Weekly_Sales', 'Weighted_Weekly_Sales']]\n",
    "store_df = store_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "unweighted_store_df = store_df[['Date', 'Store', 'Weekly_Sales']]\n",
    "\n",
    "unweighted_store_df['Store'].unique()\n",
    "\n",
    "for store_id in unweighted_store_df['Store'].unique():\n",
    "\n",
    "    unweighted_df = unweighted_store_df.loc[unweighted_store_df['Store'] == store_id]\n",
    "    unweighted_df = unweighted_df[['Date', 'Weekly_Sales']]\n",
    "\n",
    "    unweighted_df.set_index('Date', inplace=True)\n",
    "\n",
    "    y = unweighted_df['Weekly_Sales']\n",
    "\n",
    "    train_size = int(len(y) * 0.8)\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "    sarima_order = (1, 1, 1)\n",
    "    seasonal_order = (1, 1, 1, 52)  # Assuming weekly seasonality (adjust if needed)\n",
    "\n",
    "\n",
    "    model = SARIMAX(y_train, order=sarima_order, seasonal_order=seasonal_order)\n",
    "    sarima_fit = model.fit(disp=False)\n",
    "\n",
    "\n",
    "    forecast = sarima_fit.get_forecast(steps=len(y_test))\n",
    "    y_pred = forecast.predicted_mean\n",
    "    conf_int = forecast.conf_int()\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    correlation = np.corrcoef(y_test, y_pred)[0, 1]  # Correlation coefficient\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Correlation: {correlation}\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot training data\n",
    "    plt.plot(y_train.index, y_train, label='Training Data', color='blue')\n",
    "\n",
    "    # Plot actual test data\n",
    "    plt.plot(y_test.index, y_test, label='Actual Test Data', color='green')\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(y_test.index, y_pred, label='SARIMA Forecast', color='orange')\n",
    "\n",
    "\n",
    "    plt.fill_between(y_test.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)\n",
    "\n",
    "    plt.title('SARIMA: Actual vs Predicted Weekly Sales')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Weekly Sales')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Add correlation as text to the plot\n",
    "    plt.text(x=0.05, y=0.95, s=f\"Correlation: {correlation:.2f}\", fontsize=12,\n",
    "            transform=plt.gca().transAxes, color='darkred', bbox=dict(facecolor='white', alpha=0.6))\n",
    "\n",
    "    \n",
    "    filename = os.path.join(output_dir, f'Store_{store_id}_Forecasting.png')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Create an output directory to save the forecasted plots if it does not exist\n",
    "output_dir = 'TESForcastingPredictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Extract relevant columns for store-level analysis: Date, Store, Weekly Sales, and Weighted Weekly Sales\n",
    "store_df = merged_df[['Date', 'Store', 'Weekly_Sales', 'Weighted_Weekly_Sales']]\n",
    "\n",
    "# Loop through each store to generate forecasts\n",
    "for store_id in store_df['Store'].unique():\n",
    "\n",
    "    # Filter the data for the current store\n",
    "    unweighted_store_df = store_df.loc[store_df['Store'] == store_id]\n",
    "    \n",
    "    # Set 'Date' as the index for time series forecasting\n",
    "    unweighted_store_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Define and fit the Exponential Smoothing model to the store's weekly sales data\n",
    "    model = ExponentialSmoothing(\n",
    "        unweighted_store_df['Weekly_Sales'],  # Target variable (weekly sales)\n",
    "        seasonal='mul',  # Specify multiplicative seasonality (adjust based on your data's pattern)\n",
    "        seasonal_periods=52,  # Set the seasonal period to 52 for weekly seasonality (1 year)\n",
    "        trend=None  # No trend component in the model\n",
    "    )\n",
    "    fit = model.fit()  # Fit the model to the data\n",
    "    \n",
    "    # Forecast the next 52 weeks (one year)\n",
    "    forecast = fit.forecast(52)\n",
    "    \n",
    "    # Plot the original weekly sales and the forecasted values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original weekly sales data\n",
    "    plt.plot(unweighted_store_df['Weekly_Sales'], label='Original Data')\n",
    "    \n",
    "    # Plot forecasted sales for the next 52 weeks\n",
    "    plt.plot(forecast.index, forecast, label='Forecast', color='orange')\n",
    "    \n",
    "    # Add title and labels to the plot\n",
    "    plt.title(f'Weekly Sales and Forecast for Store {store_id}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Weekly Sales')\n",
    "    plt.legend()  # Display legend to differentiate between the original and forecasted data\n",
    "    \n",
    "    # Save the forecast plot for the current store as a PNG image\n",
    "    filename = os.path.join(output_dir, f'Store_{store_id}_Forecasting.png')\n",
    "    plt.savefig(filename)  # Save the figure to the specified filename\n",
    "    plt.close()  # Close the plot to free up memory before moving to the next store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'TESForcastingPredictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "store_df = merged_df[['Date', 'Store', 'Weekly_Sales', 'Weighted_Weekly_Sales']]\n",
    "\n",
    "for store_id in store_df['Store'].unique():\n",
    "\n",
    "    unweighted_store_df = store_df.loc[store_df['Store'] == store_id]\n",
    "    unweighted_store_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    model = ExponentialSmoothing(\n",
    "        unweighted_store_df['Weekly_Sales'],\n",
    "        seasonal='mul',\n",
    "        seasonal_periods=52,\n",
    "        trend = None\n",
    "        )\n",
    "    fit = model.fit()\n",
    "    forecast = fit.forecast(52)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(unweighted_store_df['Weekly_Sales'], label='Original Data')\n",
    "    plt.plot(\n",
    "        forecast.index, forecast, label='Forecast'\n",
    "        )  # Match indices of forecast with original data\n",
    "    plt.legend()\n",
    "    plt.title('Weekly Sales and Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Weekly Sales')\n",
    "    \n",
    "    filename = os.path.join(output_dir, f'Store_{store_id}_Forecasting.png')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
